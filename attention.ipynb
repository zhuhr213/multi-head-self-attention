{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import jieba\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.10.2'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, attention_dropout=0.1):\n",
    "        super(Attention, self).__init__()\n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, q, k, v, scale=False, attn_mask=None):\n",
    "        # [batch, head, seq, feature]\n",
    "        attention = torch.matmul(q, k.transpose(-1, -2))  # Q × K^T\n",
    "        if scale:\n",
    "            # print(k.shape)\n",
    "            attention = attention * math.sqrt(k.shape[-1])       # 是否设置缩放，根号dk\n",
    "        if attn_mask:\n",
    "            attention = attention.masked_fill(attn_mask, -np.inf)     # 给需要mask的地方设置一个负无穷。\n",
    "        # 计算softmax\n",
    "        attention = self.softmax(attention)  # 这里的softmax，在dim=-1, -2都是可以的\n",
    "        # 添加dropout\n",
    "        attention = self.dropout(attention)  # (N × N)\n",
    "        # softmax(Q×KT) × V\n",
    "        z = torch.matmul(attention, v)\n",
    "        return z, attention  # z是attention的最终输出，attention注意力矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, model_dim=64, num_heads=4, dropout=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.head_dim = model_dim//num_heads   # 每个头的维度\n",
    "        self.num_heads = num_heads\n",
    "        self.Wk = nn.Linear(model_dim, self.head_dim * num_heads)\n",
    "        self.Wv = nn.Linear(model_dim, self.head_dim * num_heads)\n",
    "        self.Wq = nn.Linear(model_dim, self.head_dim * num_heads)\n",
    "\n",
    "        self.Attention = Attention()\n",
    "\n",
    "        self.Wo0 = nn.Linear(model_dim, model_dim)\n",
    "        self.Wo1 = nn.Linear(model_dim, model_dim)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(model_dim)         # LayerNorm 归一化。\n",
    "\n",
    "    def forward(self, X, scale = True, attn_mask=None):\n",
    "        # 残差连接\n",
    "        residual = X\n",
    "\n",
    "        head_dim = self.head_dim     # 每个头的维度\n",
    "        num_heads = self.num_heads   # 头的个数\n",
    "        batch_size = X.shape[0]       # batch_size\n",
    "\n",
    "        # 线性映射，求  Q  K  V\n",
    "        K = self.Wk(X)\n",
    "        V = self.Wv(X)\n",
    "        Q = self.Wq(X)  # (Batch_size, seq_length, 64)\n",
    "\n",
    "        # 按照头进行分割，shape：[batch, seq, feature] -> [batch, head, seq, feature]\n",
    "        K = K.view(batch_size, num_heads, -1, head_dim)  # (Batch_size, 4, seq_length, 16)\n",
    "        V = V.view(batch_size, num_heads, -1, head_dim)\n",
    "        Q = Q.view(batch_size, num_heads, -1, head_dim)\n",
    "\n",
    "        if attn_mask:\n",
    "            attn_mask = attn_mask.repeat(num_heads, 1, 1)\n",
    "        # print(Q.shape)\n",
    "        Z, attention = self.Attention(Q, K, V, scale, attn_mask)\n",
    "\n",
    "        # 进行头合并 concat heads，又变回  [batch, seq, feature]\n",
    "        Z = Z.view(batch_size, -1, head_dim * num_heads)    # (Batch_size, seq_length, 64)\n",
    "\n",
    "        # 进行线性映射，论文中有两层线性层，激活函数是RELU\n",
    "        Z = self.Wo0(Z)\n",
    "        Z = self.relu(Z)\n",
    "        Z = self.Wo1(Z)\n",
    "        # dropout\n",
    "        Z = self.dropout(Z)\n",
    "\n",
    "        # 添加残差层和正则化层。\n",
    "        output = self.layer_norm(residual + Z)\n",
    "\n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我', '爱', '学习', '，', '学习', '使', '我', '快乐']\n",
      "['爱', '使', '学习', '我', '，', '快乐']\n",
      "{'爱': 0, '使': 1, '学习': 2, '我': 3, '，': 4, '快乐': 5}\n",
      "[3, 0, 2, 4, 2, 1, 3, 5]\n"
     ]
    }
   ],
   "source": [
    "sentence = \"我爱学习，学习使我快乐\"\n",
    "seg = jieba.cut(sentence)            # 分词\n",
    "seg_list = ','.join(seg).split(',')  # 转化为列表\n",
    "print(seg_list)\n",
    "token_list = list(set(seg_list))       # 去重\n",
    "print(token_list)\n",
    "\n",
    "dict = {}\n",
    "# for token in seg_list:\n",
    "for i in range(len(token_list)):       # 存入字典\n",
    "    dict[token_list[i]] = i\n",
    "    pass\n",
    "\n",
    "print(dict)\n",
    "\n",
    "dict['吃'] = 6\n",
    "dict['汉堡'] = 7\n",
    "seq = [dict[i] for i in seg_list]\n",
    "print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 64])\n"
     ]
    }
   ],
   "source": [
    "embedding = nn.Embedding(len(dict), 64)\n",
    "input_embedding = embedding(torch.tensor(seq)).unsqueeze(0)\n",
    "print(input_embedding.shape)   # batch_size = 1，只有1条数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 64]) torch.Size([1, 4, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "Multi_Head_Attention = MultiHeadAttention()\n",
    "output, attention = Multi_Head_Attention(input_embedding)\n",
    "print(output.shape, attention.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 64])\n"
     ]
    }
   ],
   "source": [
    "# pytorch中有封装好的transformer层，拿来用就行\n",
    "encoder_layer = nn.TransformerEncoderLayer(d_model=64, nhead=4)\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=4)\n",
    "\n",
    "output = transformer_encoder(input_embedding)\n",
    "print(output.shape)\n",
    "# decoder用法与encoder类似"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 64]) torch.Size([1, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "# pytorch还有封装好的multi-head self-attention层\n",
    "attention_layer = nn.MultiheadAttention(embed_dim=64, num_heads=4, batch_first=True)\n",
    "output, attention = attention_layer(input_embedding, input_embedding, input_embedding) #, average_attn_weights=False)\n",
    "# average_attn_weights 参数决定是否把所有head的attention取平均，默认true，如果是False，输出的attention矩阵大小就是[1,4,8,8]\n",
    "# 该参数在最新版torch(1.11)中出现。本教程使用1.10.2，因此没有该参数。\n",
    "print(output.shape, attention.shape)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4b53ac7fa5d153ec6be12cf46f2fcdd3ca80dad2178c1c954a19ccdcff0e351e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('rnabert')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
